{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.0' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Program Files/Python310/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from data_preprocessing import read_data, handle_missing_values, drop_constant_columns, fill_missing_values_with_mode,replace_dash_with_mode\n",
    "from outlier_detection import detect_outliers_zscore, remove_outliers\n",
    "from data_analysis import plot_correlation_heatmap, plot_pairplot\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, IsolationForest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    ")\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Reading Data...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Read Data\n",
    "file_path = 'dataset/http.csv'\n",
    "column_names = [\n",
    "    'timestamp', 'session_id', 'src_ip', 'src_port', 'dst_ip', 'dst_port', 'protocol', \n",
    "    'method', 'host', 'url', 'params', 'http_version', 'user_agent', 'unknown1', 'response_size',\n",
    "    'status_code', 'status_message', 'unknown2', 'unknown3', 'empty1', 'unknown4', 'unknown5',\n",
    "    'unknown6', 'unknown7', 'unknown8', 'unknown9', 'unknown10', 'token', 'unknown11', 'content_type'\n",
    "]\n",
    "\n",
    "print(\"\\nStep 1: Reading Data...\")\n",
    "data = read_data(file_path, column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2: Handling Missing Values and Dropping Constant Columns...\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Preprocess Data\n",
    "print(\"\\nStep 2: Handling Missing Values and Dropping Constant Columns...\")\n",
    "data = handle_missing_values(data, threshold=0.5)\n",
    "data = drop_constant_columns(data, threshold=0.9)\n",
    "data = fill_missing_values_with_mode(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3: Detecting and Removing Outliers...\n",
      "Preprocessed data saved as 'result/preprocessed_http_cleaned.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Detect and Remove Outliers\n",
    "print(\"\\nStep 3: Detecting and Removing Outliers...\")\n",
    "outliers = detect_outliers_zscore(data, threshold=3)\n",
    "data_cleaned = remove_outliers(data, outliers)\n",
    "\n",
    "# Save cleaned data\n",
    "data_cleaned.to_csv('result/preprocessed_http_cleaned.csv', index=False)\n",
    "print(\"Preprocessed data saved as 'result/preprocessed_http_cleaned.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Analyze Patterns and Correlations (currently disabled)\n",
    "# print(\"\\nStep 4: Analyzing Patterns and Correlations...\")\n",
    "# plot_correlation_heatmap(data_cleaned)\n",
    "# plot_pairplot(data_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 5: Processing 'dataset/httpnormal.csv'...\n",
      "Preprocessed 'dataset/httpnormal.csv' saved as 'result/preprocessed_httpnormal_cleaned.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Process Second File\n",
    "print(\"\\nStep 5: Processing 'dataset/httpnormal.csv'...\")\n",
    "file_path_normal = 'dataset/httpnormal.csv'\n",
    "data_normal = read_data(file_path_normal, column_names)\n",
    "\n",
    "\n",
    "# Align columns and preprocess\n",
    "data_normal = data_normal[data_cleaned.columns]\n",
    "data_normal = fill_missing_values_with_mode(data_normal)\n",
    "data_normal = replace_dash_with_mode(data_normal)\n",
    "data_normal.to_csv('result/preprocessed_httpnormal_cleaned.csv', index=False)\n",
    "print(\"Preprocessed 'dataset/httpnormal.csv' saved as 'result/preprocessed_httpnormal_cleaned.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 6: Balancing and Combining Data...\n",
      "Balanced final data saved as 'result/final_http_data_balanced.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Balance and Combine Data\n",
    "print(\"\\nStep 6: Balancing and Combining Data...\")\n",
    "num_normal = len(data_normal)\n",
    "data_cleaned_sampled = data_cleaned.sample(n=num_normal, random_state=42)\n",
    "\n",
    "data_cleaned_sampled['attack'] = 1\n",
    "data_normal['attack'] = 0\n",
    "\n",
    "final_data = pd.concat([data_cleaned_sampled, data_normal], axis=0)\n",
    "final_data.to_csv('result/final_http_data_balanced.csv', index=False)\n",
    "print(\"Balanced final data saved as 'result/final_http_data_balanced.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Reading Balanced Data...\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Read Balanced Data\n",
    "print(\"Step 1: Reading Balanced Data...\")\n",
    "file_path = 'result/final_http_data_balanced.csv'\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2: Preprocessing Data...\n",
      "Remaining Columns: Index(['src_ip', 'src_port', 'dst_ip', 'host', 'url', 'user_agent',\n",
      "       'response_size', 'status_code', 'attack'],\n",
      "      dtype='object')\n",
      "\n",
      "Encoding categorical variables...\n",
      "\n",
      "Step 2.1: Correlation Analysis...\n",
      "Top Correlated Features:\n",
      "attack           1.00\n",
      "status_code      0.98\n",
      "user_agent       0.79\n",
      "src_ip           0.15\n",
      "dst_ip           0.14\n",
      "response_size   -0.05\n",
      "host            -0.12\n",
      "url             -0.20\n",
      "src_port        -0.24\n",
      "Name: attack, dtype: float64\n",
      "\n",
      "Scaling numerical data...\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Preprocess Data\n",
    "print(\"\\nStep 2: Preprocessing Data...\")\n",
    "# Drop unnecessary columns (timestamp, session_id, unknowns)\n",
    "columns_to_drop = ['timestamp', 'session_id', 'status_message'] + [\n",
    "    col for col in data.columns if 'unknown' in col\n",
    "]\n",
    "data = data.drop(columns=columns_to_drop, axis=1)\n",
    "\n",
    "print(\"Remaining Columns:\", data.columns)\n",
    "\n",
    "# Encode Categorical Variables\n",
    "print(\"\\nEncoding categorical variables...\")\n",
    "categorical_cols = ['src_ip', 'dst_ip', 'protocol', 'method', 'host', 'url', 'user_agent', 'content_type']\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in data.columns:\n",
    "        data[col] = label_encoder.fit_transform(data[col])\n",
    "\n",
    "# Correlation Analysis\n",
    "print(\"\\nStep 2.1: Correlation Analysis...\")\n",
    "correlation_matrix = data.corr()\n",
    "print(\"Top Correlated Features:\")\n",
    "print(correlation_matrix['attack'].sort_values(ascending=False))\n",
    "\n",
    "# Scale Numerical Data\n",
    "print(\"\\nScaling numerical data...\")\n",
    "scaler = StandardScaler()\n",
    "numerical_cols = ['src_port', 'dst_ip', 'response_size', 'status_code']\n",
    "\n",
    "# Avoid Data Leakage: fit scaler only on the training set\n",
    "X = data.drop(columns=['attack'])\n",
    "y = data['attack']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3: Comparing Models using Lazy Predict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 31/32 [00:01<00:00, 26.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 845, number of negative: 855\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000193 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 850\n",
      "[LightGBM] [Info] Number of data points in the train set: 1700, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497059 -> initscore=-0.011765\n",
      "[LightGBM] [Info] Start training from score -0.011765\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:01<00:00, 19.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
      "Model                                                                           \n",
      "AdaBoostClassifier                 1.00               1.00     1.00      1.00   \n",
      "RandomForestClassifier             1.00               1.00     1.00      1.00   \n",
      "BaggingClassifier                  1.00               1.00     1.00      1.00   \n",
      "DecisionTreeClassifier             1.00               1.00     1.00      1.00   \n",
      "LabelSpreading                     1.00               1.00     1.00      1.00   \n",
      "ExtraTreeClassifier                1.00               1.00     1.00      1.00   \n",
      "ExtraTreesClassifier               1.00               1.00     1.00      1.00   \n",
      "LabelPropagation                   1.00               1.00     1.00      1.00   \n",
      "KNeighborsClassifier               1.00               1.00     1.00      1.00   \n",
      "LGBMClassifier                     1.00               1.00     1.00      1.00   \n",
      "SGDClassifier                      1.00               1.00     1.00      1.00   \n",
      "Perceptron                         1.00               1.00     1.00      1.00   \n",
      "CalibratedClassifierCV             0.99               0.99     0.99      0.99   \n",
      "LogisticRegression                 0.99               0.99     0.99      0.99   \n",
      "PassiveAggressiveClassifier        0.99               0.99     0.99      0.99   \n",
      "BernoulliNB                        0.99               0.99     0.99      0.99   \n",
      "SVC                                0.99               0.99     0.99      0.99   \n",
      "LinearSVC                          0.99               0.99     0.99      0.99   \n",
      "GaussianNB                         0.99               0.99     0.99      0.99   \n",
      "LinearDiscriminantAnalysis         0.99               0.99     0.99      0.99   \n",
      "RidgeClassifier                    0.99               0.99     0.99      0.99   \n",
      "RidgeClassifierCV                  0.99               0.99     0.99      0.99   \n",
      "NuSVC                              0.99               0.99     0.99      0.99   \n",
      "QuadraticDiscriminantAnalysis      0.99               0.99     0.99      0.99   \n",
      "NearestCentroid                    0.97               0.97     0.97      0.97   \n",
      "DummyClassifier                    0.49               0.50     0.50      0.32   \n",
      "\n",
      "                               Time Taken  \n",
      "Model                                      \n",
      "AdaBoostClassifier                   0.14  \n",
      "RandomForestClassifier               0.18  \n",
      "BaggingClassifier                    0.04  \n",
      "DecisionTreeClassifier               0.01  \n",
      "LabelSpreading                       0.14  \n",
      "ExtraTreeClassifier                  0.01  \n",
      "ExtraTreesClassifier                 0.12  \n",
      "LabelPropagation                     0.10  \n",
      "KNeighborsClassifier                 0.04  \n",
      "LGBMClassifier                       0.33  \n",
      "SGDClassifier                        0.02  \n",
      "Perceptron                           0.02  \n",
      "CalibratedClassifierCV               0.04  \n",
      "LogisticRegression                   0.02  \n",
      "PassiveAggressiveClassifier          0.02  \n",
      "BernoulliNB                          0.02  \n",
      "SVC                                  0.02  \n",
      "LinearSVC                            0.01  \n",
      "GaussianNB                           0.02  \n",
      "LinearDiscriminantAnalysis           0.03  \n",
      "RidgeClassifier                      0.02  \n",
      "RidgeClassifierCV                    0.02  \n",
      "NuSVC                                0.11  \n",
      "QuadraticDiscriminantAnalysis        0.02  \n",
      "NearestCentroid                      0.02  \n",
      "DummyClassifier                      0.01  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Lazy Predict to Compare Models\n",
    "print(\"\\nStep 3: Comparing Models using Lazy Predict...\")\n",
    "clf = LazyClassifier()\n",
    "models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4: Fine-Tuning Random Forest Model with Cost Sensitivity...\n",
      "\n",
      "Random Forest Model Results:\n",
      "Accuracy: 1.0\n",
      "F1 Score: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       208\n",
      "           1       1.00      1.00      1.00       218\n",
      "\n",
      "    accuracy                           1.00       426\n",
      "   macro avg       1.00      1.00      1.00       426\n",
      "weighted avg       1.00      1.00      1.00       426\n",
      "\n",
      "\n",
      "Cross-Validation for Random Forest...\n",
      "Cross-Validation F1 Scores: [0.98383372 1.         1.         1.         0.99297424]\n",
      "Mean F1 Score: 0.9953615914241365\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Fine-Tune Random Forest with Cost Sensitivity\n",
    "print(\"\\nStep 4: Fine-Tuning Random Forest Model with Cost Sensitivity...\")\n",
    "rf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='f1')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "print(\"\\nRandom Forest Model Results:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Cross-Validation for Random Forest\n",
    "print(\"\\nCross-Validation for Random Forest...\")\n",
    "rf_cv_scores = cross_val_score(best_rf, X, y, cv=5, scoring='f1')\n",
    "print(\"Cross-Validation F1 Scores:\", rf_cv_scores)\n",
    "print(\"Mean F1 Score:\", rf_cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 5: Implementing Ensemble Method with Cost Sensitivity...\n",
      "\n",
      "Ensemble Model Results:\n",
      "Accuracy: 0.9976525821596244\n",
      "F1 Score: 0.9977011494252873\n",
      "Precision: 1.0\n",
      "Recall: 0.9954128440366973\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       208\n",
      "           1       1.00      1.00      1.00       218\n",
      "\n",
      "    accuracy                           1.00       426\n",
      "   macro avg       1.00      1.00      1.00       426\n",
      "weighted avg       1.00      1.00      1.00       426\n",
      "\n",
      "\n",
      "Cross-Validation for Ensemble Model...\n",
      "Cross-Validation F1 Scores: [0.96818182 0.99528302 0.99764706 0.99763593 0.99297424]\n",
      "Mean F1 Score: 0.9903444137110593\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Ensemble Method (Voting Classifier)\n",
    "print(\"\\nStep 5: Implementing Ensemble Method with Cost Sensitivity...\")\n",
    "lr = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "svm = SVC(probability=True, class_weight='balanced')\n",
    "\n",
    "ensemble = VotingClassifier(estimators=[\n",
    "    ('rf', best_rf),\n",
    "    ('lr', lr),\n",
    "    ('svm', svm)\n",
    "], voting='soft')\n",
    "\n",
    "ensemble.fit(X_train, y_train)\n",
    "y_pred_ensemble = ensemble.predict(X_test)\n",
    "\n",
    "print(\"\\nEnsemble Model Results:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_ensemble))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_ensemble))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_ensemble))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_ensemble))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_ensemble))\n",
    "\n",
    "# Cross-Validation for Ensemble\n",
    "print(\"\\nCross-Validation for Ensemble Model...\")\n",
    "ensemble_cv_scores = cross_val_score(ensemble, X, y, cv=5, scoring='f1')\n",
    "print(\"Cross-Validation F1 Scores:\", ensemble_cv_scores)\n",
    "print(\"Mean F1 Score:\", ensemble_cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 6: Outlier Analysis using Isolation Forest...\n",
      "Outliers Detected:\n",
      "anomaly\n",
      " 1    2104\n",
      "-1      22\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Outlier Analysis and Anomaly Detection\n",
    "print(\"\\nStep 6: Outlier Analysis using Isolation Forest...\")\n",
    "iso_forest = IsolationForest(contamination=0.01, random_state=42)\n",
    "outliers = iso_forest.fit_predict(X)\n",
    "data['anomaly'] = outliers\n",
    "\n",
    "print(\"Outliers Detected:\")\n",
    "print(data['anomaly'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 7: Saving Processed Data...\n",
      "Processed data saved to 'result/processed_http_data.csv'\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Save Final Preprocessed Data (Optional)\n",
    "print(\"\\nStep 7: Saving Processed Data...\")\n",
    "data.to_csv('result/processed_http_data.csv', index=False)\n",
    "print(\"Processed data saved to 'result/processed_http_data.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
